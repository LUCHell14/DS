Practical 1a 

Aim: Design a simple linear neural network model. 

x = float(input("Enter value of x: "))
w = float(input("Enter value of weight w: "))
b = float(input("Enter value of bias b: "))

net = w * x + b

if net < 0:
    out = 0
elif net >= 0 and net <= 1:
    out = net
else:
    out = 1

print("net =", net)
print("output =", out)




1b: Calculate the output of neural net using both binary and bipolar sigmoidal function. 
For the network shown in the figure 1, calculate the net input to output neuron. 

n = int(input("Enter number of elements: "))

print("Enter the inputs")
inputs = []
for i in range(n):
    ele = float(input())
    inputs.append(ele)
print(inputs)

print("Enter the weights")
weights = []
for i in range(n):
    ele = float(input())
    weights.append(ele)
print(weights)

print("The net input can be calculated as   Yin = x1w1 + x2w2 + x3w3")
Yin = []
for i in range(n):
    Yin.append(inputs[i] * weights[i])

print(round(sum(Yin), 3))




1. Problem statement : 
1. Calculate the net input for the network shown in Figure 2 with bias included in the 
network.

n = int(input("Enter number of elements: "))

print("Enter the inputs:")
inputs = []
for i in range(n):
    ele = float(input())
    inputs.append(ele)
print(inputs)

print("Enter the weights:")
weights = []
for i in range(n):
    ele = float(input())
    weights.append(ele)
print(weights)

b = float(input("Enter bias value: "))
print("The net input can be calculated as   Yin = b + x1w1 + x2w2:")

Yin = []
for i in range(n):
    Yin.append(inputs[i] * weights[i])

print(round((sum(Yin) + b), 3))






Practical 2a: 

Aim: Implement AND/NOT function using McCulloch-Pits neuron (use binary data 
representation).

num_ip = int(input("Enter the number of inputs : "))

w1 = 1
w2 = 1

x1 = []
x2 = []

print("For the", num_ip, "inputs calculate the net input using yin = x1w1 + x2w2")

for j in range(num_ip):
    ele1 = int(input("x1 = "))
    ele2 = int(input("x2 = "))
    x1.append(ele1)
    x2.append(ele2)

n = [val * w1 for val in x1]
m = [val * w2 for val in x2]

Yin = []
for i in range(num_ip):
    Yin.append(n[i] + m[i])
print("Yin =", Yin)

Yin = []
for i in range(num_ip):
    Yin.append(n[i] - m[i])
print("After assuming one weight as excitatory and the other as inhibitory, Yin =", Yin)

Y = []
for i in range(num_ip):
    if Yin[i] >= 1:
        ele = 1
    else:
        ele = 0
    Y.append(ele)
    
    
    
    Practical 2b: 
Aim: Generate XOR function using McCulloch-Pitts neural net 


import numpy as np

print('Enter weights')
w11 = int(input('Weight w11 = '))
w12 = int(input('Weight w12 = '))
w21 = int(input('Weight w21 = '))
w22 = int(input('Weight w22 = '))
v1 = int(input('Weight v1 = '))
v2 = int(input('Weight v2 = '))

print('Enter Threshold Value')
theta = int(input('theta = '))

x1 = np.array([0, 0, 1, 1])
x2 = np.array([0, 1, 0, 1])
z = np.array([0, 1, 1, 0])

con = 1
y1 = np.zeros((4,))
y2 = np.zeros((4,))
y = np.zeros((4,))

while con == 1:
    zin1 = x1 * w11 + x2 * w21
    zin2 = x1 * w12 + x2 * w22

    print("z1", zin1)
    print("z2", zin2)

    for i in range(4):
        if zin1[i] >= theta:
            y1[i] = 1
        else:
            y1[i] = 0

        if zin2[i] >= theta:
            y2[i] = 1
        else:
            y2[i] = 0

    yin = y1 * v1 + y2 * v2

    for i in range(4):
        if yin[i] >= theta:
            y[i] = 1
        else:
            y[i] = 0

    print("yin", yin)
    print("Output of Net")
    y = y.astype(int)
    print("y", y)
    print("z", z)

    if np.array_equal(y, z):
        con = 0
    else:
        print("Net is not learning enter another set of weights and Threshold value")
        w11 = int(input("Weight w11 = "))
        w12 = int(input("Weight w12 = "))
        w21 = int(input("Weight w21 = "))
        w22 = int(input("Weight w22 = "))
        v1 = int(input("Weight v1 = "))
        v2 = int(input("Weight v2 = "))
        theta = int(input("theta = "))

print("McCulloch-Pitts Net for XOR function")
print("Weights of Neuron Z1")
print(w11)
print(w21)
print("Weights of Neuron Z2")
print(w12)
print(w22)
print("Weights of Neuron Y")
print(v1)
print(v2)
print("Threshold value")
print(theta)

  
  
  
  
Practical 3a. 
Aim: Write a program to implement Hebbâ€™s rule.


import numpy as np

x1 = np.array([1, 1, 1, -1, 1, -1, 1, 1, 1])
x2 = np.array([1, 1, 1, 1, -1, 1, 1, 1, 1])

b = 0

y = np.array([1, -1])

wtold = np.zeros((9,))
wtnew = np.zeros((9,))

wtnew = wtnew.astype(int)
wtold = wtold.astype(int)

print("First input with target = 1")
for i in range(0, 9):
    wtold[i] = wtold[i] + x1[i] * y[0]
wtnew = wtold
b = b + y[0]

print("new wt =", wtnew)
print("Bias value", b)

print("Second input with target = -1")
for i in range(0, 9):
    wtnew[i] = wtold[i] + x2[i] * y[1]
b = b + y[1]

print("new wt =", wtnew)
print("Bias value", b)




Practical 3b: 
Aim: Write a program to implement of delta rule. 

import numpy as np
import time

np.set_printoptions(precision=2)

x = np.zeros((3,))
weights = np.zeros((3,))
desired = np.zeros((3,))
actual = np.zeros((3,))

for i in range(0, 3):
    x[i] = float(input("Initial inputs: "))

for i in range(0, 3):
    weights[i] = float(input("Initial weights: "))

for i in range(0, 3):
    desired[i] = float(input("Desired output: "))

a = float(input("Enter learning rate: "))

actual = x * weights

print("actual", actual)
print("desired", desired)

while True:
    if np.array_equal(desired, actual):
        break  # no change needed
    else:
        for i in range(0, 3):
            weights[i] = weights[i] + a * (desired[i] - actual[i])
    actual = x * weights
    print("weights", weights)
    print("actual", actual)
    print("desired", desired)

print("*" * 30)
print("Final output")
print("Corrected weights", weights)
print("actual", actual)
print("desired", desired)





Practical 4a: 

Aim: Write a program for Back Propagation Algorithm   

import numpy as np
import math

np.set_printoptions(precision=2)

v1 = np.array([0.6, 0.3])
v2 = np.array([-0.1, 0.4])
w = np.array([-0.2, 0.4, 0.1])
b1 = 0.3
b2 = 0.5
x1 = 0
x2 = 1
alpha = 0.25

print("calculate net input to z1 layer")
zin1 = round(b1 + x1 * v1[0] + x2 * v2[0], 4)
print("z1 =", round(zin1, 3))

print("calculate net input to z2 layer")
zin2 = round(b2 + x1 * v1[1] + x2 * v2[1], 4)
print("z2 =", round(zin2, 4))

print("Apply activation function to calculate output")
z1 = 1 / (1 + math.exp(-zin1))
z1 = round(z1, 4)
z2 = 1 / (1 + math.exp(-zin2))
z2 = round(z2, 4)

print("z1 =", z1)
print("z2 =", z2)

print("calculate net input to output layer")
yin = w[0] + z1 * w[1] + z2 * w[2]
print("yin =", yin)

print("calculate net output")
y = 1 / (1 + math.exp(-yin))
print("y =", y)

fyin = y * (1 - y)
dk = (1 - y) * fyin
print("dk", dk)

dw1 = alpha * dk * z1
dw2 = alpha * dk * z2
dw0 = alpha * dk

print("compute error portion in delta")
din1 = dk * w[1]
din2 = dk * w[2]

print("din1 =", din1)
print("din2 =", din2)

print("error in delta")
fzin1 = z1 * (1 - z1)
d1 = din1 * fzin1
fzin2 = z2 * (1 - z2)
d2 = din2 * fzin2

print("d1 =", d1)
print("d2 =", d2)

print("Changes in weights")
dv11 = alpha * d1 * x1
dv21 = alpha * d1 * x2
dv01 = alpha * d1

dv12 = alpha * d2 * x1
dv22 = alpha * d2 * x2
dv02 = alpha * d2

v1[0] = v1[0] + dv11
v1[1] = v1[1] + dv12

v2[0] = v2[0] + dv21
v2[1] = v2[1] + dv22

w[1] = w[1] + dw1
w[2] = w[2] + dw2

b1 = b1 + dv01
b2 = b2 + dv02
w[0] = w[0] + dw0

print("v1 =", v1)
print("v2 =", v2)
print("w =", w)
print("bias b1 =", b1, " b2 =", b2)






Practical 4b 

Aim: Write a Program For Error Back Propagation Algorithm (Ebpa) Learning

import math

a0 = -1
t = -1

w10 = float(input("Enter weight first network: "))
b10 = float(input("Enter base first network: "))
w20 = float(input("Enter weight second network: "))
b20 = float(input("Enter base second network: "))
c = float(input("Enter learning coefficient: "))

n1 = float(w10 * a0 + b10)
a1 = math.tanh(n1)

n2 = float(w20 * a1 + b20)
a2 = math.tanh(n2)

e = t - a2
s2 = -2 * (1 - a2 * a2) * e
s1 = (1 - a1 * a1) * w20 * s2

w21 = w20 - (c * s2 * a1)
w11 = w10 - (c * s1 * a0)
b21 = b20 - (c * s2)
b11 = b10 - (c * s1)

print("Updated weight w11 =", w11)
print("Updated weight w21 =", w21)
print("Updated base b11 =", b11)
print("Updated base b21 =", b21)





Practical 5b: 

Aim:Write a program for Radial Basis function

import numpy as np
from numpy import mgrid
from numpy.linalg import pinv
from matplotlib import pyplot as plt

class RBF:
    def __init__(self, indim, numCenters, outdim):
        self.indim = indim
        self.outdim = outdim
        self.numCenters = numCenters
        self.centers = np.random.uniform(-1, 1, (numCenters, indim))
        self.beta = 8
        self.W = np.random.random((self.numCenters, self.outdim))

    def _basisfunc(self, c, d):
        return np.exp(-self.beta * np.linalg.norm(c - d)**2)

    def _calcAct(self, X):
        G = np.zeros((X.shape[0], self.numCenters))
        for ci, c in enumerate(self.centers):
            for xi, x in enumerate(X):
                G[xi, ci] = self._basisfunc(c, x)
        return G

    def train(self, X, Y):
        rnd_idx = np.random.permutation(X.shape[0])[:self.numCenters]
        self.centers = X[rnd_idx]

        print("Centers:", self.centers)

        G = self._calcAct(X)
        print("G Matrix:\n", G)

        self.W = np.dot(pinv(G), Y)

    def test(self, X):
        G = self._calcAct(X)
        Y = np.dot(G, self.W)
        return Y

if __name__ == '__main__':
    n = 100
    x = mgrid[-1:1:complex(0, n)].reshape(n, 1)

    # FIX: Use numpy sin
    y = np.sin(3 * (x + 0.5)**3 - 1)

    rbf = RBF(1, 10, 1)
    rbf.train(x, y)
    z = rbf.test(x)

    plt.figure(figsize=(12, 8))
    plt.plot(x, y, 'k-', label='Original')
    plt.plot(x, z, 'r-', linewidth=2, label='RBF Output')

    plt.scatter(rbf.centers, np.zeros(rbf.numCenters), color='green', label='Centers')

    plt.legend()
    plt.show()







Practical 6a:  

Aim:Self-Organizing Maps

from minisom import MiniSom
import matplotlib.pyplot as plt

data = [
    [0.80, 0.55, 0.22, 0.03],
    [0.82, 0.50, 0.23, 0.03],
    [0.80, 0.54, 0.22, 0.03],
    [0.80, 0.53, 0.26, 0.03],
    [0.79, 0.56, 0.22, 0.03],
    [0.75, 0.60, 0.25, 0.03],
    [0.77, 0.59, 0.22, 0.03]
]

som = MiniSom(6, 6, 4, sigma=0.3, learning_rate=0.5)
som.train_random(data, 100)

plt.imshow(som.distance_map())
plt.show()




Practical 7a:

Aim: Line Separation

import numpy as np
import matplotlib.pyplot as plt

def create_distance_function(a, b, c):
    def distance(xp, yp):
        nom = a * xp + b * yp + c
        if nom == 0:
            pos = 0
        elif (nom < 0 and b < 0) or (nom > 0 and b > 0):
            pos = -1
        else:
            pos = 1
        d = np.absolute(nom) / np.sqrt(a ** 2 + b ** 2)
        return (d, pos)
    return distance

points = [(3.5, 1.8), (1.1, 3.9)]

fig, ax = plt.subplots()
ax.set_xlabel("sweetness")
ax.set_ylabel("sourness")
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 8])

size = 10
for idx, (px, py) in enumerate(points):
    if idx == 0:
        ax.plot(px, py, "o", color="darkorange", markersize=size)
    else:
        ax.plot(px, py, "oy", markersize=size)

X = np.arange(-0.5, 5, 0.1)
step = 0.05

for t in np.arange(0, 1 + step, step):
    slope = np.tan(np.arccos(t))
    dist_fn = create_distance_function(slope, -1, 0)
    Y = slope * X

    results = []
    for p in points:
        results.append(dist_fn(*p))

    if results[0][1] != results[1][1]:
        ax.plot(X, Y, "g-")
    else:
        ax.plot(X, Y, "r-")

plt.show()



Practical 7b: 

Aim: Hopfield Network model of associative memory 

import numpy as np
import matplotlib.pyplot as plt

def sign(x):
    return np.where(x >= 0, 1, -1)

def train(patterns):
    n = patterns.shape[1]
    W = np.zeros((n, n))
    for p in patterns:
        W += np.outer(p, p)
    np.fill_diagonal(W, 0)
    return W

def recall(W, pattern, steps=5):
    s = pattern.copy()
    for _ in range(steps):
        s = sign(W @ s)
    return s

def plot_pattern(p, title):
    plt.imshow(p.reshape(5, 5), cmap="gray")
    plt.title(title)
    plt.axis("off")
    plt.show()

p1 = np.array([1, -1, 1, -1, 1,
               -1, 1, -1, 1, -1,
               1, -1, 1, -1, 1,
               -1, 1, -1, 1, -1,
               1, -1, 1, -1, 1])

p2 = np.array([-1, -1, 1, 1, -1,
               1, 1, -1, -1, 1,
               -1, -1, 1, 1, -1,
               1, 1, -1, -1, 1,
               -1, -1, 1, 1, -1])

patterns = np.array([p1, p2])
W = train(patterns)

noisy = p1.copy()
flip_idx = np.random.choice(25, 5, replace=False)
noisy[flip_idx] *= -1

plot_pattern(noisy, "Noisy Input")
output = recall(W, noisy, steps=6)
plot_pattern(output, "Recovered Output")





Practical 8a: 

Aim:  Membership and Identity operators in, not in. 

def overlapping(list1, list2):
    c = 0
    d = 0
    for _ in list1:
        c += 1
    for _ in list2:
        d += 1
    for i in range(c):
        for j in range(d):
            if list1[i] == list2[j]:
                return 1
    return 0

list1 = [1, 2, 3, 4, 5]
list2 = [6, 7, 8, 9]

if overlapping(list1, list2):
    print("overlapping")
else:
    print("not overlapping")
def overlapping(list1, list2):
    c = 0
    d = 0
    for _ in list1:
        c += 1
    for _ in list2:
        d += 1
    for i in range(c):
        for j in range(d):
            if list1[i] == list2[j]:
                return 1
    return 0

list1 = [1, 2, 3, 4, 5]
list2 = [6, 7, 8, 9]

if overlapping(list1, list2):
    print("overlapping")
else:
    print("not overlapping")




Practical 8b: Membership and Identity Operators is, is not 


x = 5
if type(x) is int:
    print("true")
else:
    print("false")

x = 5.2
if type(x) is not int:
    print("true")
else:
    print("false")





Practical 9a: 
Find the ratios using fuzzy logic

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

s1 = "I love fuzzysforfuzzys"
s2 = "I am loving fuzzysforfuzzys"

print("FuzzyWuzzy Ratio:", fuzz.ratio(s1, s2))
print("FuzzyWuzzyPartialRatio:", fuzz.partial_ratio(s1, s2))
print("FuzzyWuzzyTokenSortRatio:", fuzz.token_sort_ratio(s1, s2))
print("FuzzyWuzzyTokenSetRatio:", fuzz.token_set_ratio(s1, s2))
print("FuzzyWuzzyWRatio:", fuzz.WRatio(s1, s2))

query = "fuzzys for fuzzys"
choices = ["fuzzy for fuzzy", "fuzzy fuzzy", "g. for fuzzys"]

print("List of ratios:")
print(process.extract(query, choices))

print("Best among the above list:", process.extractOne(query, choices))






Practical 9b:  
Aim: Solve Tipping Problem using fuzzy logic 


import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

quality = ctrl.Antecedent(np.arange(0, 11, 1), 'quality')
service = ctrl.Antecedent(np.arange(0, 11, 1), 'service')
tip = ctrl.Consequent(np.arange(0, 26, 1), 'tip')

quality.automf(3)
service.automf(3)

tip['low'] = fuzz.trimf(tip.universe, [0, 0, 13])
tip['medium'] = fuzz.trimf(tip.universe, [0, 13, 25])
tip['high'] = fuzz.trimf(tip.universe, [13, 25, 25])

quality['average'].view()
service.view()
tip.view()

rule1 = ctrl.Rule(quality['poor'] | service['poor'], tip['low'])
rule2 = ctrl.Rule(service['average'], tip['medium'])
rule3 = ctrl.Rule(service['good'] | quality['good'], tip['high'])

rule1.view()

tipping_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])
tipping = ctrl.ControlSystemSimulation(tipping_ctrl)

tipping.input['quality'] = 6.5
tipping.input['service'] = 9.8

tipping.compute()

print(tipping.output['tip'])

tip.view(sim=tipping)






Practical 10:   
Aim: Implementation of simple genetic algorithm 




Practical 10b: 
Aim:Create two classes: City and Fitness using Genetic algorithm 
First create a City class that will allow us to create and handle our cities. 



import numpy as np
import random
import operator
import pandas as pd
import matplotlib.pyplot as plt

class City:
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def distance(self, city):
        xDis = abs(self.x - city.x)
        yDis = abs(self.y - city.y)
        distance = np.sqrt((xDis ** 2) + (yDis ** 2))
        return distance

    def __repr__(self):
        return "(" + str(self.x) + "," + str(self.y) + ")"


class Fitness:
    def __init__(self, route):
        self.route = route
        self.distance = 0
        self.fitness = 0.0

    def routeDistance(self):
        if self.distance == 0:
            pathDistance = 0
            for i in range(0, len(self.route)):
                fromCity = self.route[i]
                if i + 1 < len(self.route):
                    toCity = self.route[i + 1]
                else:
                    toCity = self.route[0]
                pathDistance += fromCity.distance(toCity)
            self.distance = pathDistance
        return self.distance

    def routeFitness(self):
        if self.fitness == 0:
            self.fitness = 1 / float(self.routeDistance())
        return self.fitness


def createRoute(cityList):
    return random.sample(cityList, len(cityList))


def initialPopulation(popSize, cityList):
    population = []
    for _ in range(popSize):
        population.append(createRoute(cityList))
    return population


def rankRoutes(population):
    fitnessResults = {}
    for i in range(len(population)):
        fitnessResults[i] = Fitness(population[i]).routeFitness()
    return sorted(fitnessResults.items(), key=operator.itemgetter(1), reverse=True)


def selection(popRanked, eliteSize):
    selectionResults = []
    df = pd.DataFrame(np.array(popRanked), columns=["Index", "Fitness"])
    df['cum_sum'] = df.Fitness.cumsum()
    df['cum_perc'] = 100 * df.cum_sum / df.Fitness.sum()

    for i in range(eliteSize):
        selectionResults.append(popRanked[i][0])

    for _ in range(len(popRanked) - eliteSize):
        pick = 100 * random.random()
        for i in range(len(popRanked)):
            if pick <= df.iat[i, 3]:
                selectionResults.append(popRanked[i][0])
                break

    return selectionResults


def matingPool(population, selectionResults):
    matingpool = []
    for i in selectionResults:
        matingpool.append(population[i])
    return matingpool


def breed(parent1, parent2):
    child = []
    childP1 = []
    geneA = int(random.random() * len(parent1))
    geneB = int(random.random() * len(parent1))
    startGene = min(geneA, geneB)
    endGene = max(geneA, geneB)

    for i in range(startGene, endGene):
        childP1.append(parent1[i])

    childP2 = [item for item in parent2 if item not in childP1]
    child = childP1 + childP2
    return child


def breedPopulation(matingpool, eliteSize):
    children = []
    length = len(matingpool) - eliteSize
    pool = random.sample(matingpool, len(matingpool))

    for i in range(eliteSize):
        children.append(matingpool[i])

    for i in range(length):
        child = breed(pool[i], pool[len(matingpool) - i - 1])
        children.append(child)

    return children


def mutate(individual, mutationRate):
    for swapped in range(len(individual)):
        if random.random() < mutationRate:
            swapWith = int(random.random() * len(individual))
            individual[swapped], individual[swapWith] = individual[swapWith], individual[swapped]
    return individual


def mutatePopulation(population, mutationRate):
    mutatedPop = []
    for ind in population:
        mutatedPop.append(mutate(ind, mutationRate))
    return mutatedPop


def nextGeneration(currentGen, eliteSize, mutationRate):
    popRanked = rankRoutes(currentGen)
    selectionResults = selection(popRanked, eliteSize)
    matingpool = matingPool(currentGen, selectionResults)
    children = breedPopulation(matingpool, eliteSize)
    nextGen = mutatePopulation(children, mutationRate)
    return nextGen


def geneticAlgorithm(population, popSize, eliteSize, mutationRate, generations):
    pop = initialPopulation(popSize, population)
    print("Initial distance: " + str(1 / rankRoutes(pop)[0][1]))

    for _ in range(generations):
        pop = nextGeneration(pop, eliteSize, mutationRate)

    print("Final distance: " + str(1 / rankRoutes(pop)[0][1]))
    bestRouteIndex = rankRoutes(pop)[0][0]
    bestRoute = pop[bestRouteIndex]
    return bestRoute


def geneticAlgorithmPlot(population, popSize, eliteSize, mutationRate, generations):
    pop = initialPopulation(popSize, population)
    progress = []
    progress.append(1 / rankRoutes(pop)[0][1])

    for _ in range(generations):
        pop = nextGeneration(pop, eliteSize, mutationRate)
        progress.append(1 / rankRoutes(pop)[0][1])

    plt.plot(progress)
    plt.ylabel('Distance')
    plt.xlabel('Generation')
    plt.show()


def main():
    cityList = []
    for _ in range(25):
        cityList.append(City(x=int(random.random() * 200),
                             y=int(random.random() * 200)))

    geneticAlgorithmPlot(
        population=cityList,
        popSize=100,
        eliteSize=20,
        mutationRate=0.01,
        generations=500
    )


if __name__ == '__main__':
    main()



























print("Output Y =", Y)
